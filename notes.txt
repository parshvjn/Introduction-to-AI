Ai terminology/more

To check if the ai is doing good you check the accuracy and error, if less error then accuracy will go more and other way around too

After Eda and data visualization we move into building an ai model
After the data set has been explored  the data set are divided into 2 parts, training data set and testing data set
In supervised learning we further divide the data set into features which are the attributes of the data or the columns with which an ai model is going to learn
Target: Column to be predicted 


Points to remember:
The number of data points or rows in training data set has to be greater than the testing data set
The data set should be divided into 60:40 ratio, 70:30, or 80:20, the greater ratio needs to be for the training data set and the smaller one needs to be for the testing data set 
Ideally the training data set should contain 60 percent of the data points or numbers of rows, another 20 percent has to be the testing data set,  and the rest 20 percent has to be validation data set
Ai model tests its accuracy itself by validating the validation data test, will only proceed if accuracy high and error low, if the other way around then it will stop and loop around

Predicting different sets of values in regression or predicting different sets of values we use the continuous data set where as in classification we use discrete data set

Ai can’t read text so you have to convert those to numbers

If values in a column are empty or NaN and it is not specified that the column is median or average, then you would jus fill the values with the means

Mean and median is known for filling data

If your data is completely positive, not neg value(wrong or bad values not literally negative) then the ai model won’t be good
So normally we add some wrong values so ai can differentiate which ones are wrong and which ones are good

Data imbalancement is a scenario where one group of data is more that the other group of data for example the number of data points for a particular group of data can be 70 percent of the entire data set where as another group of data contains only 30 percent of the entire data set.

Date points can be called weights, and bigger numbers cause ai to take longer

Normalizing is to lower the weights  (reducing the numbers)
Scaling does that too but with only a part the data

Scaling vs. Normalization: 
	Scaling means transforming of data so that it fits within a specific scale; Scaling becomes necessary when dealing with datasets containing features that have different ranges, units of measurement or order of magnitude (the values). In such cases the variation in the feature value can lead to biast model performance or difficulties in the learning process 
	In general we normalize the data to change the observation of higher magnitude to lower magnitude irrespective of any range. In scaling we are change the range of data while in normalization we are changing the shape of the distribution of the data 


Bias: An intercept or effect from an origin

Formulas to get the slope or y-int from deviations of a dataset

slope:
Sum of product of deviation(x and y)
Sum of square of deviation of X

Y-int:
mean(Y) - [m*mean(x)]

Error Evaluation in regressions
1. Mean Absolute Error

